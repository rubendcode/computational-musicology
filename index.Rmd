---
title: "Corpus Ruben Dijksma"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    self_contained: false
    source: https://github.com/rubendcode/computational-musicology/blob/main/index.Rmd
    css: css_file.css
    theme:
      heading_font:
        google: 
          family: Rajdhani
          wght: 700
      base_font:
        google: Fira Sans
      code_font:
        google: Fira Mono
      bg: "#FFFFFF"
      fg: "#212529" 
      primary: "#2b2bee"
      secondary: "#39d7b8"
      success: "#39d7b8"
      danger: "#fa5577"
      warning: "#ffb14c"
      info: "#0cc7f1"
date: "2024-02-23"
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(plotly)
library(heatmaply)
library(protoclust)
library(cowplot)
library(spotifyr)
library(htmltools)
library(compmus)
library(gridExtra)
library(cowplot)

knitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE)
```

### Introduction

When do we call a pop song an indiepop song? Last month, the famous 2000’s pop song ‘Murder on the Dancefloor’ by Sophie Ellis-Bextor has been rediscovered by TikTok after the movie Saltburn came out. The song was covered by ‘indie-pop’ duo Royel Otis. After the cover the song turned into a timeless indiepop song with sounds from the 2010’s. As a musician, myself heavily inspired by the indie-pop style I am more than interested to discover what musical components make a regular pop song differ from an indie-pop song that makes me and many others feel a certain nostalgic way. My corpus is genre-based but I plan to analyze artists, songs, playlists, and charts with this research question. I hope to be able to discover when Spotify recognizes songs within Spotify as ‘indie-pop’, ‘pop’, or maybe in some cases both. I am not sure how deep we can analyze the song's components (chords, scales etc.) or just the metadata around it. I plan to get the tracks from playlists as “Indie Pop”, “Indie Pop Hits” and “Pop Rising” curated by Spotify. As specific tracks I want to look at current “pop” hits such as “Flowers” by Miley Cyrus and “Greedy” by Tate McRae and current “indie pop” hits such as the Murder On The Dancefloor cover by Royel Otis and a classic as Kilby Girl by The Backseat Lovers.

------------------------------------------------------------------------

This index will show multiple graphs and discussions about the difference in genre.

### Energy vs Acousticness

Week 7 assignment:

For the first graphs I made 2 graphs with the energy on the X axis and the acousticness on the Y axis. I used an indie pop and pop party playlist with both graphs showing that indie pop is more acoustic and pop more energetic. These graphs already show an interesting contrast in the genres.

```{r echo=FALSE, out.width = '100%', fig.width = 4, fig.height = 6}
library(tidyverse)
library(spotifyr)

# Fetch audio features for playlists
indiepop <- get_playlist_audio_features("", "37i9dQZF1DWWEcRhUVtL8n?si=8549c3d965e4458c&nd=1&dlsi=7e635eb660464a36")
pop <- get_playlist_audio_features("", "37i9dQZF1DWXti3N4Wp5xy?si=e0a5344cb1b545c5&nd=1&dlsi=61886b9565ee4c74")

# Combine playlists into a single data frame
music <- bind_rows(
  indiepop %>% mutate(category = "Indie Pop Playlist"),
  pop %>% mutate(category = "Pop Playlist")
)

# Create the ggplot object
ggplot_obj <- ggplot(music, aes(x = energy, y = acousticness)) +
  geom_point(aes(color = category), alpha = 0.7) +
  facet_wrap(~category) +
  labs(x = "Energy", y = "Acousticness") +
  theme_minimal()

# Render the plot using ggplotly to ensure proper display
div(
  ggplotly(ggplot_obj),
  style = "width: 900px; height: 600px; overflow: scroll;"
)

```

------------------------------------------------------------------------

For these graphs I used these playlists at the moment:

```{r echo=FALSE}
library(htmltools)

div(
  HTML('<iframe src="https://open.spotify.com/embed/playlist/37i9dQZF1DWWEcRhUVtL8n?utm_source=generator" width="100%" height="390" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>')
)

div(
  HTML('<iframe src="https://open.spotify.com/embed/playlist/37i9dQZF1DWXti3N4Wp5xy?utm_source=generator" width="100%" height="390" frameBorder="0" allowfullscreen="" allow="autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture"></iframe>')
)

```

### Chromagrams Murder On The Dancefloor

```{r chromagrams, echo=FALSE}


library(cowplot)

royelotis <- get_tidy_audio_analysis("1swz9stsbG1p34SJHJqiww") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches) |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Royel Otis Murder On The Dancefloor", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()

Sophie <- get_tidy_audio_analysis("4tKGFmENO69tZR9ahgZu48") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches) |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Sophie Ellis-Bextor Murder On The Dancefloor", x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()


# Arrange the plots in a grid layout with synchronized x-axis scales
grid <- plot_grid(royelotis, Sophie, nrow = 2, align = "hv", scale = c(1, 1)) # Adjust the relative width here

# Display the combined plot
grid


```

------------------------------------------------------------------------

The chromagrams show that both tracks are in the same key. I think it is interesting to see that the E note is really present in the Royel Otis (indie) version and less in the version from Sophie. The C# is being played more (as a harmonic I assume) in the Sophie version.

### Timbre Graphs

```{r timbres, echo=FALSE}

royelotis_timbre <-
  get_tidy_audio_analysis("1swz9stsbG1p34SJHJqiww") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

royelotis_timbre |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  labs(title = "Royel Otis Murder On The Dancefloor", x = "Time (s)", y = NULL, fill = "Magnitude") +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")


sophie_timbre <-
  get_tidy_audio_analysis("4tKGFmENO69tZR9ahgZu48") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

sophie_timbre |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  labs(title = "Sophie Ellis-Bextor Murder On The Dancefloor", x = "Time (s)", y = NULL, fill = "Magnitude") +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "")


library(cowplot)

# Arrange the timbre graphs in a grid layout
grid <- plot_grid(royelotis_timbre, sophie_timbre, ncol = 2)

# Display the combined plot
grid


```

------------------------------------------------------------------------

These show that there is 'more' timbre in the original version from Sophie. This might be because the Royel Otis song was recorded as a live song while the original pop song was recorded in a proper studio. 

### Chordograms of Corpus

```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

royel_otis <-
  get_tidy_audio_analysis("1swz9stsbG1p34SJHJqiww") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

royel_otis |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
    geom_tile() +
  labs(title = "Chordogram Royel Otis Murder On The Dancefloor", x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

sophie <-
  get_tidy_audio_analysis("4tKGFmENO69tZR9ahgZu48") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

sophie |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  labs(title = "Chordogram Sophie Ellis-Bextor Murder On The Dancefloor", x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")


```

------------------------------------------------------------------------

The chordogram is not able to recognise the chords in the indiepop song as well as in the pop song.

### Playlist SD Comparisons

```{r}
library(ggplot2)
library(gridExtra)

# Load necessary libraries
library(dplyr)

# Define function to fetch playlist audio features and add audio analysis
get_and_analyze_playlist <- function(playlist_id, playlist_name) {
  playlist_features <- get_playlist_audio_features("thesoundsofspotify", playlist_id) %>%
    slice(1:50) %>%
    add_audio_analysis() %>%
    mutate(playlist_name = playlist_name)
  return(playlist_features)
}

# Function to extract keys and modes from audio features
extract_keys_and_modes <- function(audio_features) {
  # Convert integer key values to character key names
  key_names <- c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B")
  keys <- factor(key_names[audio_features$key])
  modes <- factor(ifelse(audio_features$mode == 0, "Minor", "Major"))
  return(list(keys = keys, modes = modes))
}

# Fetch audio features and analyze playlists for pop and indiepop
pop <- get_and_analyze_playlist("37i9dQZF1DWXti3N4Wp5xy", "Pop")
indiepop <- get_and_analyze_playlist("37i9dQZF1DWWEcRhUVtL8n", "Indie Pop")

# Extract keys and modes for each playlist
pop_keys_modes <- extract_keys_and_modes(pop)
indiepop_keys_modes <- extract_keys_and_modes(indiepop)

# Define all 12 possible keys
all_keys <- factor(c("C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"))

# Make sure both playlists have the same number of keys
num_keys <- max(length(pop_keys_modes$keys), length(indiepop_keys_modes$keys))
pop_keys <- rep(pop_keys_modes$keys, length.out = num_keys)
pop_modes <- rep(pop_keys_modes$modes, length.out = num_keys)
indiepop_keys <- rep(indiepop_keys_modes$keys, length.out = num_keys)
indiepop_modes <- rep(indiepop_keys_modes$modes, length.out = num_keys)

# Create data frames for Pop and Indie Pop playlists
pop_df <- data.frame(Key = factor(pop_keys, levels = all_keys), Mode = pop_modes, Playlist = "Pop")
indiepop_df <- data.frame(Key = factor(indiepop_keys, levels = all_keys), Mode = indiepop_modes, Playlist = "Indie Pop")

# Combine data frames
combined_df <- rbind(pop_df, indiepop_df)

# Plot histograms for major keys
major_df <- combined_df %>% filter(Mode == "Major")
plot_major <- ggplot(major_df, aes(x = Key, fill = Playlist)) +
  geom_bar(stat = "count", position = "dodge", width = 0.7) +
  labs(
    x = "Key",
    y = "Frequency",
    title = "Histogram of Major Keys for Pop and Indie Pop Playlists"
  ) +
  scale_fill_manual(values = c("Pop" = "skyblue", "Indie Pop" = "salmon")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot histograms for minor keys
minor_df <- combined_df %>% filter(Mode == "Minor")
plot_minor <- ggplot(minor_df, aes(x = Key, fill = Playlist)) +
  geom_bar(stat = "count", position = "dodge", width = 0.7) +
  labs(
    x = "Key",
    y = "Frequency",
    title = "Histogram of Minor Keys for Pop and Indie Pop Playlists"
  ) +
  scale_fill_manual(values = c("Pop" = "skyblue", "Indie Pop" = "salmon")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Assuming combined_df contains the combined data for both playlists
# Assuming combined_df contains the combined data for both playlists

# Count the number of major and minor keys in each playlist
major_minor_count_per_playlist <- combined_df %>%
  group_by(Playlist, Mode) %>%
  summarise(Count = n()) %>%
  pivot_wider(names_from = Mode, values_from = Count, values_fill = list(Count = 0))


# Arrange plots in a grid
grid <- grid.arrange(plot_major, plot_minor, ncol = 1)


```

------------------------------------------------------------------------
The chart for major keys shows that C major (C) is the most frequent key in both pop and indie pop playlists, followed by G major (G) and A major (A). There is a higher frequency of major keys in both pop and indie pop playlists compared to minor keys.

The chart for minor keys shows that A minor (Am) is the most frequent key in both pop and indie pop playlists, followed by G minor (Gm) and Em minor (Em). Overall, the data suggests that major keys are more common in both pop and indie pop playlists than minor keys. This aligns with the general perception of pop and indie pop music as being upbeat and positive.

There seem to be no indie pop songs in the key of A# major and minor in the sample. Just like there are more songs in minor on the A, F# and E minor scale.

### Novelty Functions

```{r }

# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming you have the data for the two playlists stored in pop_df and indiepop_df

pop_party <- get_and_analyze_playlist("37i9dQZF1DWXti3N4Wp5xy", "Pop Party")
fresh_pop <- get_and_analyze_playlist("37i9dQZF1DX2fMaj5GfMh3", "Fresh Pop")

indiepop <- get_and_analyze_playlist("37i9dQZF1DWWEcRhUVtL8n", "Indie Pop")
diff_indie <- get_and_analyze_playlist("37i9dQZF1DXad2sxzzYX1N", "Indie That Hits Different")

# Combine the data into one dataframe
combined_df <- rbind(
  data.frame(Playlist = "Pop Party", tempo = pop_party$tempo),
  data.frame(Playlist = "Fresh Pop", tempo = fresh_pop$tempo),
  data.frame(Playlist = "Indie Pop", tempo = indiepop$tempo),
  data.frame(Playlist = "Indie That Hits Different", tempo = diff_indie$tempo)
)

# Create boxplot to compare tempo across playlists
ggplot(combined_df, aes(x = Playlist, y = tempo, fill = Playlist)) +
  geom_boxplot() +
  labs(
    title = "Comparison of Tempo Across Playlists",
    x = "Playlist",
    y = "Tempo"
  ) +
  scale_fill_manual(values = c("skyblue", "salmon", "green", "purple")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


***

To see if these pop and indiepop playlists are that different from each other I decided to add two more playlists (Fresh Pop and Indie That Hits Different). It is interesting to see that the range of the two indie pop playlists is really similar and the two pop playlists are too. It seems like there is a correlation in this difference in the two genres. 
Q1 and Q3 are in a wider range from each other in both playlists while in the Pop playlists they are closer to each other. 

The Pop playlists have multiple outliers that are either around 175 or 80. It seems like this is the case since the average is closer to each other Q1 and Q3. In the next page there are Tempograms to see this better.

### Time Signature in different playlists

```{r }

# Load necessary libraries
library(ggplot2)
library(dplyr)

# Assuming you have the data for the two playlists stored in pop_df and indiepop_df
# I'll just create sample data for demonstration purposes

pop_party <- get_and_analyze_playlist("37i9dQZF1DWXti3N4Wp5xy", "Pop Party")
fresh_pop <- get_and_analyze_playlist("37i9dQZF1DX2fMaj5GfMh3", "Fresh Pop")
indiepop <- get_and_analyze_playlist("37i9dQZF1DWWEcRhUVtL8n", "Indie Pop")
diff_indie <- get_and_analyze_playlist("37i9dQZF1DXad2sxzzYX1N", "Indie That Hits Different")

# Combine the data into one dataframe
combined_df <- rbind(
  data.frame(Playlist = "Pop Party", time_signature = pop_party$time_signature),
  data.frame(Playlist = "Fresh Pop", time_signature = fresh_pop$time_signature),
  data.frame(Playlist = "Indie Pop", time_signature = indiepop$time_signature),
  data.frame(Playlist = "Indie That Hits Different", time_signature = diff_indie$time_signature)
)

# Create histogram to compare time signatures across playlists
ggplot(combined_df, aes(x = time_signature, fill = Playlist)) +
  geom_histogram(binwidth = 0.5, position = "dodge") +
  labs(
    title = "Comparison of Time Signatures Across Playlists",
    x = "Time Signature",
    y = "Count"
  ) +
  scale_fill_manual(values = c("skyblue", "salmon", "green", "purple")) +
  theme_minimal()

```
*** 

I wanted to see if there are big differences in time signatures within both genres. 
There are a few more songs in 3/4 in indie pop then there are in pop playlists. 
I would have expected this difference to be bigger.

### Tempogram Outliers In Pop Playlists

```{r }
drunktext <- get_tidy_audio_analysis("0KpWiHVmIFDTvai20likX4")
alibi <- get_tidy_audio_analysis("4JyS3WGxalmpzgEbVyTycL")
blinding_lights <- get_tidy_audio_analysis("0VjIjW4GlUZAMYd2vXMi3b")

drunktext |>
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(title = "drunk text - Henry Moodie", x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

alibi |>
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(title = "The Alibi - Dylan", x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

blinding_lights |>
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(title = "Blinding Lights - The Weeknd", x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()

```

*** 

In the boxplot analysis there were outliers in the Pop playlists. After analysing these playlists it was clear that they were indeed just outliers. The tempo of these songs are constant. Drunk text by Henry Moodie has a BPM of 186. 

What can be the reason for these songs to be outliers is that songs are getting faster and 'pop' is getting faster by the time. This could be a good question for future research. 

### Classification and Clustering

```{r }

library(ggdendro)
library(heatmaply)

library(compmus)

get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit |> 
    collect_predictions() |> 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit |> 
    conf_mat_resampled() |> 
    group_by(Prediction) |> mutate(precision = Freq / sum(Freq)) |> 
    group_by(Truth) |> mutate(recall = Freq / sum(Freq)) |> 
    ungroup() |> filter(Prediction == Truth) |> 
    select(class = Prediction, precision, recall)
}  

oyster <-
  get_playlist_audio_features("oystercollection", "37i9dQZF1DWWEcRhUVtL8n") |>
  add_audio_analysis() |>
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) |>
  mutate(pitches = map(pitches, compmus_normalise, "clr")) |>
  mutate_at(vars(pitches, timbre), map, bind_rows) |>
  unnest(cols = c(pitches, timbre))

# Select only 40 songs
oyster <- slice(oyster, 1:40)

oyster_juice <-
  recipe(
    track.name ~
      danceability +
      energy +
      loudness +
      acousticness +
      instrumentalness +
      valence +
      tempo +
      duration,
    data = oyster
  ) |>
  step_center(all_predictors()) |>
  step_scale(all_predictors()) |> 
  # step_range(all_predictors()) |> 
  prep(oyster |> mutate(track.name = str_trunc(track.name, 20))) |>
  juice() |>
  column_to_rownames("track.name")

oyster_dist <- dist(oyster_juice, method = "euclidean")

heatmaply(
  oyster_juice,
  hclustfun = hclust,
  hclust_method = "average",  # Change for single, average, or complete linkage.
  dist_method = "euclidean"
)

```

*** 

I did a random selection for clustering of 40 song of the Indie Pop playlist. 
Not suprisingly loudness and energy are clustered together. 

A song that stood out to me was is Jesus is dead that has a high instrumentalness and is alone in it's cluster. Even though indiepop songs are overall all high in instrumentalness the song gets picked up as high in instrumentalness. 


### Conclusion & Discussion

```{r }
```
